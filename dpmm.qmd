---
title: "Dirichlet Process Mixture Model"
format: html
---

R code for the model is takeb from "A tutorial on Dirichlet process mixture modeling" by Yuelin Li et al.

Model definition:
$$
\Pi_N \sim CRP(N, \alpha) 
$$

$$
\forall c \in \Pi_N; \quad \mu_c \sim \mathcal{N}(\mu_0, \Sigma_0) \text{ and let } \tau_0 = \Sigma_0^{-1}
$$

$$
\forall c \in \Pi_N, \forall n \in c; \quad Y_n \sim \mathcal{N}(\mu_c, \Sigma_y) \text{ where } \tau_y = \Sigma_y^{-1} \text{ is assumed known.}
$$

**Goal:** We want to use Gibbs Sampling to sample from the posterior distribution $p(\pi_N | Y_1, \cdots, Y_N)$. This posterior represents an updated distribution over set of all possible partitions of data. 

**Notation:**

1. `z`: $N \times 1$ vector of cluster assignments.
2. `n_k`: vector containing num. of data points in each of $k$ (which is _not_ fixed) clusters.
3. `Nclust`: total number of clusters
4. Data `y` is $D$-dimensional.

```{r}
library(mvtnorm)
```

```{r}
crp_gibbs <- function(data ,alpha = 0.01 ,mu0 ,sigma0, sigma_y, c_init, maxIters = 1000) 
{
#  data: an N by D matrix of data points
#  alpha: positive scalar that controls the rate at which clusters are formed. 
#  sigma_y: measurement error of data y, assumed known, same for all clusters.
#  
#  mu0, sigma0: prior mean and variance around unknown mean mu
#  mu0 = matrix(rep(0, data_dim), ncol = data_dim, byrow = TRUE) 
#  sigma0 = diag(data_dim) * 3^2
#
#  c_init: initial assignments of data points to clusters
#  maxIters: MC chain length; it also prevents endless loops 

# data is D-dimenstional 
data_dim <- ncol(data) 

# number of data points 
N <- nrow(data) 
 
# prior precision on the unknown mean mu.
# i.e. inverse of prior covariance 
tau0 <- solve(sigma0) 

# cluster-specific precision, assumed known, all clusters are assumed to 
# share identical measurement error of y ~ N(mu, sigma_y).
tau_y <- solve(sigma_y) 


# initialize the CRP Gibbs sampler 
z <- c_init                # initial cluster assignments  
n_k <- as.vector(table(z)) # initial data counts at each cluster. 
Nclust <- length(n_k)      # initial number of clusters 


# Chinese Restaurant Process (CRP) Gibbs sampler begins 

# cluster membership storage
# Each row corresponds to a MC chain for the n-th data point
res <- matrix(NA, nrow = N, ncol = maxIters)  

# progress bar to track progress of iterations
pb <- txtProgressBar(min = 0, max = maxIters, style = 3) 


for(iter in 1:maxIters) { 
  for(n in 1:N) {    # one data point (customer) at a time 

# what is the nth person's table/cluster assignment? 
c_i <- z[n] 
# remove the nth person from table 
n_k[c_i] <- n_k[c_i] - 1 

# if the table becomes empty when the nth person is removed, 
# then that table/cluster is removed. 
if( n_k[c_i] == 0 ) 
{ 
  n_k[c_i] <- n_k[Nclust]   # last cluster to replace this empty cluster 
  loc_z <- ( z == Nclust )  # who are in the last cluster? 
  z[loc_z] <- c_i           # move them up to fill just emptied cluster 
  n_k <- n_k[ -Nclust ]     # take out the last cluster, now empty 
  Nclust <- Nclust - 1      # decrease total number of clusters by 1 
} 


# ensures z[n] doesn't get counted as a cluster
z[n] <- -1 

# Now we are ready to update table assignment by Eqs (12) and (13). 
# log probabilities for the clusters, add previously unoccupied table 
logp <- rep( NA, Nclust + 1 ) 


# loop over already occupied tables 1:J and calculate prob as per Eq (13). 
for( c_i in 1:Nclust ) { 

  tau_p <- tau0 + n_k[c_i] * tau_y  # cluster precision as per Eq (4)
  sig_p <- solve(tau_p)             # cluster variance, inverse of tau_c

  # find all of the points in this cluster 
  loc_z <- which(z == c_i) 

  # sum all the points in this cluster 
  if(length(loc_z) > 1) { 
    sum_data <-  colSums(data[z == c_i, ]) } 
  else { 
    sum_data <-  data[z == c_i, ] 
  } 

# We need to use the predictive distribution of each already 
# occupied table to predict the next customer sitting there. 
# 
# Use Eq (4) to find the conditional posterior distribution for 
# the cluster means, then use the predictive distribution of 
# y_j in Eq (12) to predict new data value c_i from c-i. 

mean_p <-  sig_p %*% (tau_y %*% sum_data + tau0 %*% t(mu0)) 

logp[c_i] <- log(n_k[c_i]) + 
   dmvnorm(data[n,], mean = mean_p, sigma = sig_p + sigma_y, log = TRUE) 
} # for loop ends

# We are done looping over already occupied tables.
 
 
# Next, we use Eq (13) to calcualte the log probability of a previously 
# unoccupied, "new" table. Essentially, it is the prior predicitive 
# distribution of the DP.
logp[ Nclust+1 ] <- log(alpha) + dmvnorm(data[n,], mean = mu0, sigma = sigma0 + sigma_y, log = TRUE) 


# transform unnormalized log probabilities into probabilities 
max_logp <- max(logp) 
logp <- logp - max_logp 
loc_probs <- exp(logp) 
loc_probs <- loc_probs / sum(loc_probs) 

# draw a sample of which cluster this new customer should belong to 
newz <- sample(1:(Nclust+1), 1, replace = TRUE, prob = loc_probs) 


# spawn a new cluster if necessary 
if(newz == Nclust + 1) { 
  n_k <- c(n_k, 0) 
  Nclust <- Nclust + 1 
  } 

z[n] <- newz 
n_k[newz] <- n_k[newz] + 1 # update the cluster n_k 
}

# update text progress bar after each iter 
setTxtProgressBar(pb, iter) 

# cluster membership of N observations
res[, iter] <- z     
} 
close(pb)            # close text progress bar 
invisible(res)       # return results, N by maxIters matrix 
}
```



### Reference:
Li, Y., Schofield, E., and Gönen, M. (2019), “**A tutorial on Dirichlet process mixture modeling**,” _Journal of Mathematical Psychology_, Elsevier BV. \url{https://doi.org/10.1016/j.jmp.2019.04.004}. 